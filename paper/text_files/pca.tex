\par \indent To perform Principal Components Analysis (PCA), we first considered the method outlined by the guide posted on the class website. Using matrix algebra, we were able to compute principal components by finding orthogonal projections (the shortest distance between points). which has the advantage of projecting the original data so that the predictors are independent. 
\par After consulting with J.B. Poline in lecture, we took a new approach to PCA by implementing the Singular Value Decomposition (SVD) method for 4-dimensional data reshaped to 2-dimensional space. The SVD method is considerably more computationally efficient. With our large number predictors (the time intervals), the original 2-dimensional matrix may be too large to interpret or study because there would be too many correlations to look at. To analyze the data in a more meaningful form, we were able to use PCA to reduce the number of important variables to a few, interpretable linear combinations of our dataset. Though the dimensions of the dataset did not change, we were able to make our dataset easier to explore and visualize. For example, with the help of eigenvectors and eigenvalues, PCA allowed us to focus on the components with the most explained variance. 
